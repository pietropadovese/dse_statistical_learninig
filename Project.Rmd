---
title: "Project"
author: "Giordano Vitale"
date: "2023-05-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
df <- read.csv('airline_passenger_satisfaction.csv')
head(df)
```

```{r}
df <- df[, -1]
```


Consider the non-NA values only.
```{r}
df <- na.omit(df)
head(df)
```



```{r}
str(df)
```


We need to change the support of some variables, because they are expressed as categorical, qualitative but we need the quantitative support in order to compute the correlation matrix.

```{r}
df$customer_class <- ifelse(df$customer_class == "Eco Plus", "Eco", df$customer_class) 
df$satisfaction <- ifelse(df$satisfaction == "satisfied", 1, 0)
df$Gender <- ifelse(df$Gender == "Male", 1, 0)
df$customer_type <- ifelse(df$customer_type == "Loyal Customer", 1, 0)
df$customer_class <- ifelse(df$customer_class == 'Eco', 1, 0)
df$type_of_travel <- ifelse(df$type_of_travel == 'Personal Travel', 1, 0)
```

Double check we now have all the variables in numerical support.
```{r}
str(df)
```



Check if it worked correctly.
```{r}
sum(is.na(df))
```

Once checked, compute the correlation between the variable "satisfaction" and all the other ones.
```{r}
correlation <- cor(df[, -which(names(df) == "satisfaction")], df$satisfaction, use = "complete.obs")
correlation
```

Rank them for better visualization.
```{r}
# Extract the absolute values of the correlations
abs_correlation <- abs(correlation)

# Order the correlations in descending order
order_indices <- order(-abs_correlation)

# Sort the correlations and variable names accordingly
sorted_correlation <- correlation[order_indices]
sorted_variable_names <- rownames(correlation)[order_indices]

# Print the correlations and associated variable names in descending order
for (i in 1:length(sorted_correlation)) {
  print(paste("Variable name:", sorted_variable_names[i]))
  print(paste("Correlation:", sorted_correlation[i]))
  print("-----")
}
```



# Logistic Model - including all the variables


Create test and training set
```{r}
library(caret)

split_train_test <- createDataPartition(df$satisfaction, p=0.8, list=FALSE)
dtrain <- df[split_train_test,]
dtest <-  df[-split_train_test,]
```


Build the *Logistic Model* on the training set.(all variables)
```{r}
logistic_model <-glm(satisfaction ~., data=dtrain , family="binomial" )

summary <- summary (logistic_model)
round(summary$coefficients, digits = 3)
```


Predict the points of the *test set* using the Logistic Model above implemented:
```{r}
logistic_prediction <- predict(logistic_model, dtest, type="response")
```


Confusion Matrix to evaluate the goodness:
```{r}
# Set a threshold for classification
threshold <- 0.5

# Convert the predicted probabilities (through logistic model) to binary predictions based on the threshold
logistic_prediction_binary <- ifelse(logistic_prediction > threshold, 1, 0)

# Create a table of predicted values vs. actual values
confusion_matrix <- table(logistic_prediction_binary, dtest$satisfaction)

# Print the confusion table
print(confusion_matrix)
print(mean(logistic_prediction_binary==dtest$satisfaction))
```
We obtained a fraction of ~87% observation of the test set correctly predicted.



Plot the ROC Curve (https://www.displayr.com/what-is-a-roc-curve-how-to-interpret-it/)
```{r}
library(pROC)
test_roc = roc(dtest$satisfaction ~ logistic_prediction, plot = TRUE, print.auc = TRUE)
```


# Linear Discriminant Analysis

Build the *LDA* model
```{r}
library(MASS)

lda_model <- lda(satisfaction~.,data = dtrain)
lda_model
```

Plot the coefficients of LDA Model.
```{r}
plot(lda_model)
```

Predict the points of the *test set* using the *LDA Model* above implemented:
```{r}
lda_prediction = predict(lda_model, dtest)
lda_class_prediction = lda_prediction$class
# this prediction yields the class. This will be useful to compute the confusion matrix, because there we will not need the probabilities! 
```


Output the confusion matrix
```{r}
table(lda_class_prediction, dtest$satisfaction)
mean(lda_class_prediction==dtest$satisfaction)
```
We see that we correctly predicted 86% of the labels of the test set.


ROC for LDA
```{r}
lda_probabilities_prediction = lda_prediction$posterior[, "1"]  # either write 0 or 1!

# given the output produced by predict() we only take the "posterior" column, which gives us the predicted probabilities [0,1] that we need to build the ROC.
# to understand this: if you print "predict(lda_model, dtest)$posterior" we can see that we have two columns, which are complementary. So, we just need one column of them.



test_roc = roc(dtest$satisfaction ~ lda_probabilities_prediction, plot = TRUE, print.auc = TRUE)

```

# Quadratic Discriminant Analysis
```{r}
qda_model <- qda(satisfaction~.,data = dtrain)
qda_model
```

Predict the points of the *test set* using the *QDA Model* above implemented:
```{r}
qda_prediction = predict(qda_model, dtest)
qda_class_prediction = qda_prediction$class
```


Output the confusion matrix
```{r}
table(qda_class_prediction, dtest$satisfaction)
mean(qda_class_prediction==dtest$satisfaction)
```

Plot the ROC Curve
```{r}
qda_probabilities_prediction <- qda_prediction$posterior[,"1"]

test_roc = roc(dtest$satisfaction ~ qda_probabilities_prediction, plot = TRUE, print.auc = TRUE)
```


# knn

```{r}
library(class)

# create Data Domain X for the TRAINING set
train_X = dtrain[-23]

# create Data Domain X for the TEST set
test_X= dtest[-23]

# labels of the training set
train_Y = dtrain$satisfaction

# labels of the test set
test_Y = dtest$satisfaction
```
 

```{r}
library(ggplot2)

# Define the range of k values to evaluate
k_values <- c(1,3,5,7,9)

# Initialize an empty vector to store the accuracies
accuracies <- c()

# Compute the accuracy for each k value
for (k in k_values) {
  knn_prediction <- knn(train_X, test_X, train_Y, k = k)
  accuracy <- mean(knn_prediction == test_Y)
  accuracies <- c(accuracies, accuracy)
}

# Create a data frame with the k values and accuracies
acc_data <- data.frame(k = k_values, accuracy = accuracies)
```


```{r}
# Plot the elbow plot
ggplot(acc_data, aes(x = k, y = accuracy)) +
  geom_line() +
  geom_point() +
  labs(x = "k", y = "Accuracy") +
  ggtitle(" Accuracy for KNN")
```


Error Rate plot
```{r}
# Initialize an empty vector to store the error rates
error_rates <- c()
k_values <- c(1,3,5,7,9)

for (i in acc_data$accuracy){
error_rate <- 1 - i
error_rates <- c(error_rates, error_rate)
}


# Create a data frame with the k values and error rates
err_data <- data.frame(k = k_values, error_rate = error_rates)

# Plot the error rate plot
ggplot(err_data, aes(x = k, y = error_rate)) +
  geom_line() +
  geom_point() +
  labs(x = "k", y = "Error Rate") +
  ggtitle("Error Rate Plot for KNN")

```
*WE CHOOSE K=5*

in the following chunk, we add prob=TRUE because we need the estimated probabilities to build the ROC later on.
```{r}
five_nn_prediction <- knn(train_X, test_X, train_Y, k = 5, prob=TRUE)
```


```{r}
confusion_matrix_knn <- table(test_Y, five_nn_prediction)
confusion_matrix_knn

mean(five_nn_prediction==test_Y)
```

# TREE PREDICTOR

```{r}
# Convert the response variable to a factor
dtrain$satisfaction <- as.factor(dtrain$satisfaction)
dtest$satisfaction <- as.factor(dtest$satisfaction)

# Train the rpart model
tree_model <- rpart(satisfaction ~ ., data = dtrain)
```


```{r}
# Make predictions on the test set
predictions <- predict(tree_model, newdata = dtest, type = "class")

```


```{r}
accuracy <- sum(predictions == dtest$satisfaction) / nrow(dtest) * 100

# Print the accuracy
cat("Accuracy: ", accuracy, "%\n")
```


```{r}
library(rpart.plot)
rpart.plot(tree_model)
```


```{r}
library(ggplot2)

# Calculate test error rates
error_logreg <- mean(logistic_prediction_binary != test_Y)
error_lda <- mean(lda_class_prediction != test_Y)
error_qda <- mean(qda_class_prediction != test_Y)
error_5nn <- mean(five_nn_prediction != test_Y)
error_tree <- mean(predictions != test_Y)
# Create a data frame with method and error rate information
dataaaa <- data.frame(Method = c("Logistic Regression", "LDA", "QDA", "5-NN","TREE"),
                   Error_Rate = c(error_logreg, error_lda, error_qda, error_5nn, error_tree))

# Plot barplot
ggplot(dataaaa, aes(x = Method, y = Error_Rate)) +
  geom_bar(stat = "identity", fill = "Red") +
  ylab("Test Error Rate") +
  ggtitle("Barplot of Test Error Rates") +
  theme_bw()

```



# SUBSET

subset of the most explanatory coefficients

```{r}
log_mod <- glm(satisfaction ~ customer_type + type_of_travel+customer_class+online_boarding+inflight_wifi_service+checkin_service+onboard_service , data=dtrain , family="binomial")
summary1 <- summary (log_mod)
summary1
```

Prediction for the new log_mod
```{r}
log_mod_pred <- predict(log_mod, dtest, type="response")
```

```{r}

# Set a threshold for classification
threshold <- 0.5

# Convert the predicted probabilities (through logistic model) to binary predictions based on the threshold
log_mod_pred_binary <- ifelse(log_mod_pred > threshold, 1, 0)

# Create a table of predicted values vs. actual values
confusion_matrix_log_mod <- table(log_mod_pred_binary, dtest$satisfaction)

# Print the confusion table
print(confusion_matrix_log_mod)
print(mean(log_mod_pred_binary==dtest$satisfaction))
```


Plot the ROC Curve for Subset
```{r}
library(pROC)
test_roc_sub = roc(dtest$satisfaction ~ log_mod_pred, plot = TRUE, print.auc = TRUE)
```

# Linear Discriminant Analysis

Build the *LDA* model FOR SUBSET
```{r}
library(MASS)

lda_model_sub <- lda(satisfaction ~ customer_type + type_of_travel+customer_class+online_boarding+inflight_wifi_service+checkin_service+onboard_service,data = dtrain)
lda_model_sub

```
Plot the coefficients of LDA Model FOR SUBSET
```{r}
plot(lda_model_sub)
```

Predict the points of the *test set* using the *LDA Model* above implemented:
```{r}
lda_prediction_sub = predict(lda_model_sub, dtest)
lda_class_prediction_sub = lda_prediction_sub$class
# this prediction yields the class. This will be useful to compute the confusion matrix, because there we will not need the probabilities! 
```

Output the confusion matrix
```{r}
table(lda_class_prediction_sub, dtest$satisfaction)
mean(lda_class_prediction_sub==dtest$satisfaction)
```
We see that we correctly predicted 85.7% of the labels of the test set.


ROC for LDA
```{r}
lda_probabilities_prediction_sub = lda_prediction_sub$posterior[, "1"]  # either write 0 or 1!

# given the output produced by predict() we only take the "posterior" column, which gives us the predicted probabilities [0,1] that we need to build the ROC.
# to understand this: if you print "predict(lda_model, dtest)$posterior" we can see that we have two columns, which are complementary. So, we just need one column of them.



test_roc_sub = roc(dtest$satisfaction ~ lda_probabilities_prediction_sub, plot = TRUE, print.auc = TRUE)

```

# Quadratic Discriminant Analysis
```{r}
qda_model_sub <- qda(satisfaction ~ customer_type + type_of_travel+customer_class+online_boarding+inflight_wifi_service+checkin_service+onboard_service,data = dtrain)
qda_model_sub
```

Predict the points of the *test set* using the *QDA Model* above implemented:
```{r}
qda_prediction_sub = predict(qda_model_sub, dtest)
qda_class_prediction_sub = qda_prediction_sub$class
```


Output the confusion matrix
```{r}
table(qda_class_prediction_sub, dtest$satisfaction)
mean(qda_class_prediction_sub==dtest$satisfaction)
```

Plot the ROC Curve
```{r}
qda_probabilities_prediction_sub <- qda_prediction_sub$posterior[,"1"]

roc(dtest$satisfaction ~ qda_probabilities_prediction_sub, plot = TRUE, print.auc = TRUE)
```
# knn

```{r}
library(class)

# create Data Domain X for the TRAINING set
train_X_sub = dtrain[,c("customer_type", "type_of_travel","customer_class","online_boarding","inflight_wifi_service","checkin_service","onboard_service")]

# create Data Domain X for the TEST set
test_X_sub= dtest[,c("customer_type", "type_of_travel","customer_class","online_boarding","inflight_wifi_service","checkin_service","onboard_service")]

# labels of the training set
train_Y_sub = dtrain$satisfaction

# labels of the test set
test_Y_sub = dtest$satisfaction
```
 

```{r}
library(ggplot2)

# Define the range of k values to evaluate
k_values <- c(1,3,5,7,9)

# Initialize an empty vector to store the accuracies
accuracies_sub <- c()

# Compute the accuracy for each k value
for (k in k_values) {
  knn_prediction_sub <- knn(train_X_sub, test_X_sub, train_Y_sub, k = k)
  accuracy_sub <- mean(knn_prediction_sub == test_Y_sub)
  accuracies_sub <- c(accuracies_sub, accuracy_sub)
}
```


```{r}
# Create a data frame with the k values and accuracies
acc_data_sub <- data.frame(k = k_values, accuracy_sub = accuracies_sub)
```

```{r}
# Plot the elbow plot
ggplot(acc_data_sub, aes(x = k, y = accuracy_sub)) +
  geom_line() +
  geom_point() +
  labs(x = "k", y = "Accuracy") +
  ggtitle(" Accuracy for KNN sub")
```

Error Rate plot
```{r}
# Initialize an empty vector to store the error rates
error_rates_sub <- c()
k_values <- c(1,3,5,7,9)

for (i in acc_data_sub$accuracy_sub){
error_rate_sub <- 1 - i
error_rates_sub <- c(error_rates_sub, error_rate_sub)
}


# Create a data frame with the k values and error rates
err_data_sub <- data.frame(k = k_values, error_rate_sub = error_rates_sub)

# Plot the error rate plot
ggplot(err_data_sub, aes(x = k, y = error_rate_sub)) +
  geom_line() +
  geom_point() +
  labs(x = "k", y = "Error Rate") +
  ggtitle("Error Rate Plot for KNN sub")

```

*WE CHOOSE K=5*

in the following chunk, we add prob=TRUE because we need the estimated probabilities to build the ROC later on.
```{r}
five_nn_prediction_sub <- knn(train_X_sub, test_X_sub, train_Y_sub, k = 5, prob=TRUE)
```


```{r}
confusion_matrix_knn_sub <- table(test_Y_sub, five_nn_prediction_sub)
confusion_matrix_knn_sub

mean(five_nn_prediction_sub==test_Y_sub)
```
# TREE PREDICTOR sub

```{r}
library(rpart)
```

```{r}
# Convert the response variable to a factor
dtrain$satisfaction <- as.factor(dtrain$satisfaction)
dtest$satisfaction <- as.factor(dtest$satisfaction)

# Train the rpart model
tree_model_sub <- rpart(satisfaction ~ customer_type + type_of_travel+customer_class+online_boarding+inflight_wifi_service+checkin_service+onboard_service, data = dtrain)
```


```{r}
# Make predictions on the test set
predictions_sub <- predict(tree_model_sub, newdata = dtest, type = "class")

```


```{r}
accuracy_sub <- sum(predictions_sub == dtest$satisfaction) / nrow(dtest) * 100

# Print the accuracy
cat("Accuracy: ", accuracy_sub, "%\n")
```


```{r}
library(rpart.plot)
rpart.plot(tree_model_sub)
```

```{r}
library(ggplot2)

# Calculate test error rates
error_logreg_sub <- mean(log_mod_pred_binary != test_Y_sub)
error_lda_sub <- mean(lda_class_prediction_sub != test_Y_sub)
error_qda_sub <- mean(qda_class_prediction_sub != test_Y_sub)
error_5nn_sub <- mean(five_nn_prediction_sub != test_Y_sub)
error_tree_sub <- mean(predictions_sub != test_Y_sub)
# Create a data frame with method and error rate information
dataaaa_sub <- data.frame(Method_sub = c("Logistic Regression", "LDA", "QDA", "5-NN","TREE"),
                   Error_Rate_sub = c(error_logreg_sub, error_lda_sub, error_qda_sub, error_5nn_sub, error_tree_sub))

# Plot barplot
ggplot(dataaaa_sub, aes(x = Method_sub, y = Error_Rate_sub)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  ylab("Test Error Rate") +
  ggtitle("Barplot of Test Error Rates sub") +
  theme_bw()

```
```{r}
merged_data
```


```{r}
dataaaa$Dataset <- "Full Dataset"  # Add a new column to specify the dataset


dataaaa_sub$Dataset <- "Subset Dataset"  # Add a new column to specify the dataset


colnames(dataaaa_sub)[colnames(dataaaa_sub) == "Error_Rate_sub"] <- "Error_Rate"
# Combine the data frames
merged_data <- rbind(dataaaa, dataaaa_sub)

# Plot the merged barplot
ggplot(merged_data, aes(x = Method, y = Error_Rate, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  ylab("Test Error Rate") +
  ggtitle("Barplot of Test Error Rates") +
  theme_bw()

```


# interpretation 

Logistic Regression:
Error Rate: 0.12777542 (Full Dataset), 0.13796965 (Subset Dataset)
The Logistic Regression method performed slightly better on the Full Dataset with a lower error rate compared to the Subset Dataset.


LDA (Linear Discriminant Analysis):
Error Rate: 0.12959030 (Full Dataset), 0.14410936 (Subset Dataset)
Similar to Logistic Regression, LDA also had a slightly lower error rate on the Full Dataset compared to the Subset Dataset.


QDA (Quadratic Discriminant Analysis):
Error Rate: 0.14036375 (Full Dataset), 0.13928254 (Subset Dataset)
QDA had a higher error rate on the Full Dataset compared to the Subset Dataset, indicating that it performed relatively better on the Subset Dataset.


5-NN (k-Nearest Neighbors):
Error Rate: 0.24462293 (Full Dataset), 0.06691895 (Subset Dataset)
The 5-NN method had a significantly higher error rate on the Full Dataset but performed remarkably better on the Subset Dataset, with a substantially lower error rate.


TREE (Decision Tree):
Error Rate: 0.11742673 (Full Dataset), 0.11742673 (Subset Dataset)
The TREE method had the same error rate on both the Full Dataset and the Subset Dataset, indicating that it performed consistently on both datasets.


