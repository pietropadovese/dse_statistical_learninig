---
title: 'AIRLINE PASSENGERS SATISFACTION: CLASSIFICATION'
output:
  html_document:
    df_print: paged
date: "16/06/2023"
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

# Project description 
For our project we decided to analyze the "airlines customer satisfaction" dataset available on kaggle. This dataset contains information on passangers who have already flown with the airline that provided this data. These cover general characteristics of the passenger (age, gender, etc.), and of the type of flight (distance, class, etc.), feedback on different aspects of the experience the customers had, as well as their level of satisfaction in general.

As the goal of our work, we tried to extract insights that might be of interest to the company that owns the data in order to improve their service and strategy. With this idea, we structured the analysis in this way: 
1) Exploratory data analysis, to understand and visualize the data provided; 
2) Customer segmentation, in order to identify their different characteristics and enable the company to better target its efforts; 
3) Creation of a model to predict whether or not a passenger will be satisfied given the variables examined



## Variable description
* ...1: Index column
* Gender: Refers to the gender of the customer, either male or female.
* Customer Type: Indicates whether the customer is a loyal customer (someone who frequently chooses the same airline or company) or a disloyal customer (someone who doesn't consistently choose the same airline or company).
* Age: Represents the age of the customer.
* Type of Travel: Specifies the purpose of the customer's travel, such as personal travel or business travel.
* Customer Class: Indicates the class or category in which the customer traveled, such as Economy Plus or Business.
* Flight Distance: Represents the distance traveled by the customer's flight.
* Inflight Wi-Fi Service: Rates the satisfaction level of the customer with the inflight Wi-Fi service on a scale of 0 to 5.
* Departure/Arrival Time Convenience: Rates the satisfaction level of the customer with the convenience of departure and arrival times on a scale of 0 to 5.
* Ease of Online Booking: Rates the satisfaction level of the customer with the ease of booking flights online on a scale of 0 to 5.
* Gate Location: Rates the satisfaction level of the customer with the gate location at the airport on a scale of 0 to 5.
* Food and Drink: Rates the satisfaction level of the customer with the quality of food and drink provided on the flight on a scale of 0 to 5.
* Online Boarding: Rates the satisfaction level of the customer with the online boarding process on a scale of 0 to 5.
* Seat Comfort: Rates the satisfaction level of the customer with the comfort of the seats on the flight on a scale of 0 to 5.
* Inflight Entertainment: Rates the satisfaction level of the customer with the inflight entertainment options on a scale of 0 to 5.
* Onboard Service: Rates the satisfaction level of the customer with the service provided by the flight crew on a scale of 0 to 5.
* Leg Room Service: Rates the satisfaction level of the customer with the legroom space on the flight on a scale of 0 to 5.
* Baggage Handling: Rates the satisfaction level of the customer with the handling of baggage by the airline on a scale of 1 to 5.
* Check-in Service: Rates the satisfaction level of the customer with the check-in service at the airport on a scale of 0 to 5.
* Inflight Service: Rates the satisfaction level of the customer with the overall inflight service on a scale of 0 to 5.
* Cleanliness: Rates the satisfaction level of the customer with the cleanliness of the aircraft on a scale of 0 to 5.
* Departure Delay in Minutes: Specifies the number of minutes of departure delay experienced by the customer.
* Arrival Delay in Minutes: Specifies the number of minutes of arrival delay experienced by the customer.
* Satisfaction: Represents the overall satisfaction level of the customer with their travel experience, categorized as either "satisfied" or "neutral or dissatisfied"



**Rewrite the outline**

# OUTLINE 
1. **Data upload**
2. **Exploratory analysis:**
    - data cleaning
    - missing values handling
    - outliers analysis
    - descriptive analysis:
        + satisfaction distribution
        + customers information
        + survey rating variables
        + numerical variables
    - encoding
    - correlation matrix
3. **Unsupervised Learning**
    - PCA
    - Optimal number of clusters
    - K-means
4. **Logistic regression:**
    - general logistic model
    - forward step-wise models analysis
    - cross-validation
    - models comparison based on R-squared, CV error, CP
    - models selection (maybe one model per criteria if they differs)
    - models evaluation (accuracy, specificity, sensitivity, ...)
    - ROC curve
5. **Linear Discriminant Analysis:**
    - ...
    - compare LDA and logistic
6. **Tree predictors:**
    - ...
    - comparison with logistic model
7. **Conclusion and Interpretation**


## Libraries

```{r, results = FALSE, warnings = FALSE}
set.seed(123)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(grid)
library(gridExtra)
library(corrr)
library(ggradar)
library(scales)
library(tibble)
library(caret)
library(factoextra)

```


# **1) DATA**

```{r, results = FALSE, warnings = FALSE}
df <- read_csv("dataset/airline_passenger_satisfaction.csv")
```


Give more compact names to columns:

```{r}
df <- df %>% rename(gender = Gender, cust_type = customer_type,
                    trav_type = type_of_travel, 
                    class = customer_class, 
                    distance = flight_distance, 
                    wifi = inflight_wifi_service, 
                    time_conv = departure_arrival_time_convenient, 
                    onl_book = ease_of_online_booking, 
                    gate_loc = gate_location, 
                    food = food_and_drink, 
                    onl_board = online_boarding, 
                    comfort = seat_comfort, 
                    entert = inflight_entertainment, 
                    onb_serv  = onboard_service, 
                    leg_room = leg_room_service, 
                    baggage = baggage_handling, 
                    checkin = checkin_service, 
                    infl_serv = inflight_service, 
                    clean = cleanliness, 
                    dep_delay = departure_delay_in_minutes,
                    arr_delay = arrival_delay_in_minutes, 
                    sat = satisfaction)
```



The zero rating corresponds to a missing answer for that variable.
For this reason we decided to remove the rows where a zero appears.


```{r}
df_filtered <- df[!apply(df[, 8:21] == 0, 1, any), ]
df <- df_filtered 
```


Remove the index column

```{r}
df <- df[-1]
```


# **2) EXPLANATORY ANALYSIS**


## Handling missing values
```{r}
# Function that counts NA values for each column
na_counts <- sapply(df, function(x) sum(is.na(x)))


# Calculate the percentage of NA values for each column
na_percentage <- na_counts / nrow(df) * 100

# Display a table with NA counts and percentages
na_table <- data.frame(Column = names(na_counts), NA_Count = na_counts, Percentage = na_percentage, row.names = NULL)

# na_table <- na_table[-1,]
na_table
```

Missing values are present only for the column *arrival delay in minutes* for just a little bit more than 0.3% of the observations. We check if, as common sense might tell us, there is correlation between *departure delay in minutes.*
  
```{r}
correlation_dep_arr <- cor(df$dep_delay, df$arr_delay, use = "complete.obs")

correlation_dep_arr
```
 
Given the high correlation we decided to drop the arr_delay column and remove in this way the NA values. 

```{r}
df <- df %>% select(- arr_delay)
```


 
## Outliers analysis
The majority of variables take values on a fixed scale.
The variables worth checking for outliers are: *departure delay in minutes* and *flight distance*. 

```{r}
plot(df$dep_delay, main = "Departure Delay",
     xlab = "Index", ylab = "Departure delay in minutes", col = "darkgreen")

```

## 3*SD method
```{r}
z_scores_dep <- scale(df$dep_delay)

outliers_dep <- which(abs(z_scores_dep) > 3)

sd_outliers_count_dep <- length(outliers_dep)

cat("Observations that falls over 3 times the sd (departure delay):", sd_outliers_count_dep, "\n")
cat("Percentage of outliers identified (departure delay):", 
    round(sd_outliers_count_dep / length(df$dep_delay), 4) * 100, "%", "\n")

```
Over 2% of the observations are identified as outliers using the standard deviation method.

 
```{r}
plot(df$distance, main = "Flight Distance",
     xlab = "Index", ylab = "Flight distance", col = "darkblue")
```


```{r}
z_scores_dist <- scale(df$distance)

outliers_dist <- which(abs(z_scores_dist) > 3)

sd_outliers_count_dist <- length(outliers_dist)

cat("Observations that falls over 3 times the sd (flight distance):", sd_outliers_count_dist, "\n")

cat("Percentage of outliers identified (flight distance):", round(sd_outliers_count_dist / length(df$distance), 4) * 100, "%", "\n")
```

We eventually decided to not remove any outliers.

 
## Descriptive analysis

```{r, warnings = FALSE}
p_satisf <- ggplot(df, aes(x = sat)) +
  geom_bar(aes(fill = sat)) +
  geom_text(aes(y = ..count.., 
                label = paste0(round(prop.table(..count..), 4) * 100, '%')), 
            stat = 'count', 
            position = position_dodge(0.3), 
            size = 4,
            vjust = - 0.5) +
  scale_fill_manual(values = c("red", "green")) +
  labs(title = "Satisfaction Distribution", y = "Count")

p_satisf
```

Define a function to plot bar charts
```{r}
# Plot bar chart function
plot_bar_chart <- function(df, var_name) {
  var_name <- enquo(var_name)  # Defuse the var_name so it can be evaluated later

  plot_var <- ggplot(df, aes(x = !!var_name)) +  # !! to inject the var_name back to evaluate it
    geom_bar(aes(fill = sat)) +
    geom_text(aes(y = ..count.., 
                  label = paste0(round(prop.table(..count..), 4) * 100, '%')), 
              stat = 'count', 
              position = position_dodge(0.3), 
              size = 4,
              vjust = -0.5) +
    scale_fill_manual(values = c("red", "green")) +
    labs(title = paste0("Satisfaction Distribution for ", quo_name(var_name)), 
         x = quo_name(var_name), y = "Count")  # quo_name() to return the var_name as string

  print(plot_var)
}
```


### Customers information Variables
```{r}
# Gender
plot_bar_chart(df, gender)

# Age
df$age_group <- cut(df$age, breaks = seq(0, max(df$age) + 10, by = 10), right = FALSE)
plot_bar_chart(df, age_group)

# Customer type
plot_bar_chart(df, cust_type)

# Type of travel
plot_bar_chart(df, trav_type)

# Customer class
plot_bar_chart(df, class)
```


Dropping group_age because it is not needed anymore (we will only use age)

```{r}
df <- df%>% select(-age_group)
```


### Survey rating variables

```{r}
# Inflight Wifi service
plot_bar_chart(df, wifi)

# Departure/arrival time convenient
plot_bar_chart(df, time_conv)

# Ease of online booking
plot_bar_chart(df, onl_book)

# Gate location
plot_bar_chart(df, gate_loc)

# Food and drink
plot_bar_chart(df, food)

# Online boarding
plot_bar_chart(df, onl_board)

# Seat comfort
plot_bar_chart(df, comfort)

# Inflight entertainment
plot_bar_chart(df, entert)

# Onboard service
plot_bar_chart(df, onb_serv)

# Leg room service
plot_bar_chart(df, leg_room)

# Baggage handling
plot_bar_chart(df, baggage)

# Check-in service
plot_bar_chart(df, checkin)

# Inflight service
plot_bar_chart(df, infl_serv)

# Cleanliness
plot_bar_chart(df, clean)

```




### Numerical Variables
```{r}
# Flight distance
p_distance <- ggplot(data = df, aes(distance, color = sat)) +
  geom_freqpoly(binwidth = 100, linewidth = 1) +
  scale_color_manual(values = c("red", "green")) +
  labs(title = "Satisfaction distribution for Flight distance")
p_distance

# Departure delay
# We selected delays under 200 min otherwise the graph look even worse than this
p_dep_del <- ggplot(data = df[df$dep_delay < 200, ], 
                    aes(dep_delay, color = sat)) +
  geom_freqpoly(binwidth = 15, linewidth = 0.7) +
  scale_color_manual(values = c("red", "green")) +
  labs(title = "Satisfaction distribution for Departure delay")
p_dep_del

```
 
### Encoding
 
To display the correlation matrix we first encoded the variables with one-hot encoding.
 
```{r}
df$class_Eco <- ifelse(df$class =="Eco", 1, 0)
df$class_Eco_plus <- ifelse(df$class == "Eco Plus", 1, 0)
df$sat <- ifelse(df$sat == "satisfied", 1, 0)
df$gender <- ifelse(df$gender == "Male", 1, 0)
df$cust_type <- ifelse(df$cust_type == "Loyal Customer", 1, 0)
df$class <- ifelse(df$class == 'Eco', 1, 0)
df$trav_type <- ifelse(df$trav_type == 'Personal Travel', 1, 0)
df <- df %>% select(- class)
df <- df %>% relocate(class_Eco, .after = age)
df <- df %>% relocate(class_Eco_plus, .after = class_Eco)
df
```


## Correlation matrix

```{r}
correlation_matrix <- df %>%
  cor()

round(correlation_matrix, 3)
```



```{r}
library(corrplot)
corrplot(cor(correlation_matrix))
```




# CUSTOMER SEGMENTATION

The goal of this part was to use the k-means method to identify different clusters of passengers. Using the original data, however, we noticed that it is complicated to separate consumers into identifiable groups, so before proceeding with k-means, we transformed the data with PCA in order to reduce the number of dimensions, eliminate possible noise in the data, and generally make clustering operations easier. 


Drop the response variable.


```{r}
df_uns <- df %>% select(-sat)
```


Perform PCA on the data after scaling the variables 


```{r}
pca <- prcomp(df_uns, scale = TRUE)
```


```{r}
summary(pca)
```



```{r}
fviz_eig(pca, 
         addlabels = TRUE, 
         choice="eigenvalue",
         main="Figure 2",
         ncp = 22) +
         geom_hline(yintercept=1, 
         linetype="dashed", 
         color = "red") + 
        ggtitle("Eigenvalue of each component")
```

If we only had to keep the components with an eigenvalue bigger than 1, 
we could select the first 8 ones. 

But this information is not enough, because with 8 components we are able to explain
only 70% of the total variance. 

```{r}
pve <- 100 * pca$sdev^2 / sum(pca$sdev^2)
par(mfrow = c(1,2))
plot(pve, type = 'o', ylab = 'PVE', 
     xlab = "Principal Component", col = "blue")
plot(cumsum(pve), type = "o", ylab = "Cumulative PVE", 
     xlab = "Principal Component", col = "brown3")
```

Considering this trade-off we choose to keep the first eleven components. 


The next step is to determine the optimal number of clusters. 



```{r}
# Determine number of clusters
wss <- (nrow(df_uns)-1)*sum(apply(df_uns,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(df_uns,
                                     centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")
```

From the graph it seems that we can choose a number of clusters equal to 3.

Now that we have selected our hyperparameters, we can apply k-means on the first eleven components of the PCA. 

```{r}
km <- kmeans(pca$x[, 1:11], 3, nstart = 20)
plot(pca$x[,1:2], col = alpha(km$cluster + 1, 0.5), pch = 20, cex = .2)
```


Now that we have grouped the passengers into the different groups, we can take a closer look to them. 

Add the cluster column to the original df, then plot the satisfaction rate for different clusters:

```{r}
clust <- names(sort(table(km$clust)))
df$cluster = km$clust

ggplot(df, aes(x=cluster)) + geom_bar(aes(fill = as.factor(sat))) + scale_fill_manual(values = c("red", "green"), labels = c("neutral or dissatisfied", "satisfied")) +
  labs(fill = "Satisfaction")
```

Taking into consideration the percentage of satisfied passengers in each group we note that this varies considerably. 
It may therefore be useful for the company to analyze what causes this difference. 


```{r}
aggregate(df,by=list(df$cluster),FUN=mean)
```



```{r}
plot_cluster <- function(var_name){
  var_name <- sym(var_name)
  p <- ggplot(df, aes(x = !!var_name, fill = as.factor(cluster))) +  geom_bar() + ggtitle(paste0(var_name, " by cluster")) +
    theme(plot.title = element_text(size = 16)) + labs(fill = "Cluster")
  p
}
```


```{r}
xVariables <- colnames(df)[8:21]
```



```{r}
par(mfrow = c(3, 5))
lapply(xVariables, plot_cluster)
```




# Prediction Model

Remove the cluster column:


```{r}
df <- df %>% select(-cluster)
```



```{r}
split_train_test <- createDataPartition(df$sat, p=0.8, list=FALSE)
dtrain <- df[split_train_test,]
dtest <-  df[-split_train_test,]
```




# Logistic Model


```{r}
logistic_model <-glm(sat ~., data=dtrain , family="binomial" )

summary <- summary (logistic_model)
round(summary$coefficients, digits = 3)
```


Predict the points of the *test set* using the Logistic Model above implemented:
```{r}
logistic_prediction <- predict(logistic_model, dtest, type="response")
```


Confusion Matrix to evaluate the goodness:
```{r}
# Set a threshold for classification
threshold <- 0.5

# Convert the predicted probabilities (through logistic model) to binary predictions based on the threshold
logistic_prediction_binary <- ifelse(logistic_prediction > threshold, 1, 0)

# Create a table of predicted values vs. actual values
confusion_matrix <- table(logistic_prediction_binary, dtest$sat)

# Print the confusion table
print(confusion_matrix)
print(mean(logistic_prediction_binary==dtest$sat))
```

We obtained a fraction of ~91% observation of the test set correctly predicted.


Plot the ROC Curve (https://www.displayr.com/what-is-a-roc-curve-how-to-interpret-it/)
```{r}
library(pROC)
test_roc = roc(dtest$sat ~ logistic_prediction, plot = TRUE, print.auc = TRUE)
```


# Linear Discriminant Analysis

Build the *LDA* model
```{r}
library(MASS)

lda_model <- lda(sat~.,data = dtrain)
lda_model
```

Plot the coefficients of LDA Model. 
```{r}
plot(lda_model)
```


Predict the points of the *test set* using the *LDA Model* above implemented:
```{r}
lda_prediction = predict(lda_model, dtest)
lda_class_prediction = lda_prediction$class
```


Output the confusion matrix
```{r}
table(lda_class_prediction, dtest$sat)
mean(lda_class_prediction==dtest$sat)
```
We see that we correctly predicted 90%% of the labels of the test set.

ROC for LDA
```{r}
lda_probabilities_prediction = lda_prediction$posterior[, "1"]  # either write 0 or 1!

# given the output produced by predict() we only take the "posterior" column, which gives us the predicted probabilities [0,1] that we need to build the ROC.
# to understand this: if you print "predict(lda_model, dtest)$posterior" we can see that we have two columns, which are complementary. So, we just need one column of them.



test_roc = roc(dtest$sat ~ lda_probabilities_prediction, plot = TRUE, print.auc = TRUE)

```

# Quadratic Discriminant Analysis
```{r}
qda_model <- qda(sat~.,data = dtrain)
qda_model
```

Predict the points of the *test set* using the *QDA Model* above implemented:
```{r}
qda_prediction = predict(qda_model, dtest)
qda_class_prediction = qda_prediction$class
```


Output the confusion matrix
```{r}
table(qda_class_prediction, dtest$sat)
mean(qda_class_prediction==dtest$sat)
```


Plot the ROC Curve
```{r}
qda_probabilities_prediction <- qda_prediction$posterior[,"1"]

test_roc = roc(dtest$sat ~ qda_probabilities_prediction, plot = TRUE, print.auc = TRUE)
```


# knn


```{r}
library(class)

# create Data Domain X for the TRAINING set
train_X = dtrain[-23]

# create Data Domain X for the TEST set
test_X= dtest[-23]

# labels of the training set
train_Y = dtrain$sat

# labels of the test set
test_Y = dtest$sat
```


```{r}
library(ggplot2)

# Define the range of k values to evaluate
k_values <- c(1,3,5,7,9)

# Initialize an empty vector to store the accuracies
accuracies <- c()

# Compute the accuracy for each k value
for (k in k_values) {
  knn_prediction <- knn(train_X, test_X, train_Y, k = k)
  accuracy <- mean(knn_prediction == test_Y)
  accuracies <- c(accuracies, accuracy)
}

# Create a data frame with the k values and accuracies
acc_data <- data.frame(k = k_values, accuracy = accuracies)
```


```{r}
# Plot the elbow plot
ggplot(acc_data, aes(x = k, y = accuracy)) +
  geom_line() +
  geom_point() +
  labs(x = "k", y = "Accuracy") +
  ggtitle(" Accuracy for KNN")
```


Error Rate plot
```{r}
# Initialize an empty vector to store the error rates
error_rates <- c()
k_values <- c(1,3,5,7,9)

for (i in acc_data$accuracy){
error_rate <- 1 - i
error_rates <- c(error_rates, error_rate)
}


# Create a data frame with the k values and error rates
err_data <- data.frame(k = k_values, error_rate = error_rates)

# Plot the error rate plot
ggplot(err_data, aes(x = k, y = error_rate)) +
  geom_line() +
  geom_point() +
  labs(x = "k", y = "Error Rate") +
  ggtitle("Error Rate Plot for KNN")

```
*WE CHOOSE K=5*

in the following chunk, we add prob=TRUE because we need the estimated probabilities to build the ROC later on.

```{r}
five_nn_prediction <- knn(train_X, test_X, train_Y, k = 5, prob=TRUE)
```


```{r}
confusion_matrix_knn <- table(test_Y, five_nn_prediction)
confusion_matrix_knn

mean(five_nn_prediction==test_Y)
```

# TREE PREDICTOR

```{r}
library(rpart)
# Convert the response variable to a factor
dtrain$sat <- as.factor(dtrain$sat)
dtest$sat <- as.factor(dtest$sat)

# Train the rpart model
tree_model <- rpart(sat ~ ., data = dtrain)
```


```{r}
# Make predictions on the test set
predictions <- predict(tree_model, newdata = dtest, type = "class")

```


```{r}
accuracy <- sum(predictions == dtest$sat) / nrow(dtest) * 100

# Print the accuracy
cat("Accuracy: ", accuracy, "%\n")
```

```{r}
library(rpart.plot)
rpart.plot(tree_model)
```



```{r}

# Calculate test error rates
error_logreg <- mean(logistic_prediction_binary != test_Y)
error_lda <- mean(lda_class_prediction != test_Y)
error_qda <- mean(qda_class_prediction != test_Y)
error_5nn <- mean(five_nn_prediction != test_Y)
error_tree <- mean(predictions != test_Y)
# Create a data frame with method and error rate information
dataaaa <- data.frame(Method = c("Logistic Regression", "LDA", "QDA", "5-NN","TREE"),
                   Error_Rate = c(error_logreg, error_lda, error_qda, error_5nn, error_tree))

# Plot barplot
ggplot(dataaaa, aes(x = Method, y = Error_Rate)) +
  geom_bar(stat = "identity", fill = "Red") +
  ylab("Test Error Rate") +
  ggtitle("Barplot of Test Error Rates") +
  theme_bw()

```
